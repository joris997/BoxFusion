dataset: online
model_path: ./models/cutr_rgbd.pth
clip_path: ./models/open_clip_pytorch_model.bin
class_features: './data/class_features.pt'
class_txt: ./data/panoptic_categories_nomerge.txt
viz_on_gt_points: True
device: cuda
total_duration: 10

recipe: scenario_1

data:
  datadir: /home/none/c_space_stl_results/
  start: 0 
  output_dir: /home/none/c_space_stl_results/
  gap: 25


# cam:
#   H: 480
#   W: 640
#   fx: 320.0
#   fy: 320.0
#   cx: 319.5
#   cy: 239.5
#   png_depth_scale: 1000.
# cam:
#   H: 480
#   W: 640
#   fx: 607.2999
#   fy: 607.3384
#   cx: 319.2763
#   cy: 235.4929
#   png_depth_scale: 1000

# Azure Kinect DK
rgb_cam: # scaled from 2048x1536
  H: 480
  W: 640
  fx: 303.2797
  fy: 303.1887
  cx: 315.6464
  cy: 242.3771
depth_cam: # scaled from 1024x1024
  H: 480
  W: 640
  fx: 303.2797
  fy: 303.1887
  cx: 315.6464
  cy: 242.3771
  png_depth_scale: 1000.0

# Azure Kinect proper calibration
# rgb_cam:
rgb_cam: # scaled from 2048x1536
  H: 480
  W: 640
  fx: 307.39
  fy: 300.12
  cx: 320.00
  cy: 240.00
depth_cam: # scaled from 1024x1024
  H: 480
  W: 640
  fx: 307.39
  fy: 300.12
  cx: 320.00
  cy: 240.00
  png_depth_scale: 1000.0

detection:
  score_thresh: 0.2 #0.4 #0.25
  uv_bound: True
  uv_bound_value: 0.9
  floor_mask: True #False
  floor_ratio: 15
  scale_box: 1.0
  workspace_filter: True
  workspace:
    x: [0.0, 1.0]
    y: [-0.5, 0.5]
    z: [0.0, 1.0]

association:
  small_threshold: 0.2 #0.2
  rotation_gap: 30 #30
  translation_gap: 0.8 #1.0 #0.8 #0.5 #0.5 best #1.0 #0.1


box_fusion:
  use: True 
  iters: 20 
  pst_path: ./data/pst_1024_0.tiff
  pst_size: 1024
  random_opt:
    center_init_size: 0.1 
    center_scaling_coefficient:  0.1
    shape_init_size: 0.5
    shape_scaling_coefficient: 0.5 
  check_valid: False
  nms_threshold: 0.1 
  small_size: 0.35


vis:
  rerun: False
  show_class: True 
  show_label: True 
  trajectory: True

eval: False